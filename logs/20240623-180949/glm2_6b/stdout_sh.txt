执行的文件名：/home/lihao/run_LLM_web_ui.sh
第一个参数为：/home/lihao/git/chatglm2-6b
第二个参数为：25386
INFO 06-23 18:10:15 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='/home/lihao/git/chatglm2-6b', speculative_config=None, tokenizer='/home/lihao/git/chatglm2-6b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/home/lihao/git/chatglm2-6b)
WARNING 06-23 18:10:16 tokenizer.py:126] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 06-23 18:10:16 utils.py:451] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 06-23 18:10:29 model_runner.py:146] Loading model weights took 11.6608 GB
INFO 06-23 18:10:35 gpu_executor.py:83] # GPU blocks: 11976, # CPU blocks: 9362
INFO 06-23 18:10:36 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-23 18:10:36 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-23 18:10:44 model_runner.py:924] Graph capturing finished in 8 secs.
Running on local URL:  http://0.0.0.0:25386

To create a public link, set `share=True` in `launch()`.
执行的文件名：/home/lihao/run_LLM_web_ui.sh
第一个参数为：/home/lihao/git/chatglm2-6b
第二个参数为：25386
INFO 06-23 18:18:27 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='/home/lihao/git/chatglm2-6b', speculative_config=None, tokenizer='/home/lihao/git/chatglm2-6b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/home/lihao/git/chatglm2-6b)
WARNING 06-23 18:18:27 tokenizer.py:126] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.
WARNING 06-23 18:18:27 utils.py:451] Using 'pin_memory=False' as WSL is detected. This may slow down the performance.
INFO 06-23 18:18:38 model_runner.py:146] Loading model weights took 11.6608 GB
INFO 06-23 18:18:44 gpu_executor.py:83] # GPU blocks: 11976, # CPU blocks: 9362
INFO 06-23 18:18:44 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 06-23 18:18:44 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 06-23 18:18:50 model_runner.py:924] Graph capturing finished in 6 secs.
Running on local URL:  http://0.0.0.0:25386

To create a public link, set `share=True` in `launch()`.
